{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59a14d6-8693-49e6-ab9f-d4ee82ccdfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                                                                                                                                                                                   \n",
    "                                                                                                                                                                                                                    \n",
    "df = pd.read_csv('/Users/thinhtruong/workspace/lexical-negation/results.csv')                                                                                                                                         \n",
    "                                                                                                                                                                                                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d8252f-93af-4381-a680-d5976db60fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102177a5-f713-4170-9984-c408a1990296",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {'gpt2': 'GPT-2', 'cl100k_base': 'GPT-4', 'roberta-base': 'RoBERTa-base', 'google/flan-t5-xxl': 'Flan-T5-xxl',\n",
    "                'google/electra-base-discriminator': 'ELECTRA-base', 'albert-base-v2': 'ALBERT', 'bert-base-uncased': 'BERT-base',\n",
    "              'meta-llama/Llama-2-13b-chat-hf':'LLama-2-13B','xlnet-base-cased': 'XLNet-base'\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6fe8eb-c4cb-44da-a850-a33d42532c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenizer'] = df['tokenizer'].map(lambda x: model_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e93f37-fd2a-4ac0-bdd9-84201da02511",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cafb5ee-0fc3-4961-a49a-d9cec4488f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_negmorph.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1855054-ed1c-42dd-9c4c-a465dee9eecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flan_over = df.loc[ (df['tokenizer'] == 'Flan-T5-xxl') & (df['NegMorph'] == 'Over-segmented') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f3ce9-5722-4893-939f-b339b703207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flan_over"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3c2c22-b247-4c27-9aaf-3b283af79729",
   "metadata": {},
   "source": [
    "# Error analysis\n",
    "plot percentage correct/incorrect prediction for each affix for each token \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2043652d-8282-423b-b1cf-9c746d73612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df.groupby(['tokenizer', 'negative_affix'])['Predict'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314fb2e6-31c2-483a-b288-2aeae7decd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90717b6b-3303-4476-9509-bc279b8b812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path ='/Users/thinhtruong/workspace/lexical-negation/affix_incorrect_tokenized_into.csv'                                                                                                                         \n",
    "                                                                                                                                                                                                                    \n",
    "# Load the dataset                                                                                                                                                                                                    \n",
    "data = pd.read_csv(file_path)                                                                                                                                                                                         \n",
    "                                                                                                                                                                                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b0aa9-5a91-4853-af56-2989a4a92dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['tokenizer'] == 'LLama-2-13B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d188c4c3-ffe7-4590-b1ad-d38448d39509",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, row in df.iterrows():\n",
    "    tokenizer = row['tokenizer']\n",
    "    word = row['neg_word']\n",
    "    negative_affix = row['negative_affix']\n",
    "    # print(word, tokenizer)\n",
    "    row_tmp = data.loc[ (data['tokenizer'] == tokenizer) & (data['neg_word'] == word)]\n",
    "    tokenized_into = row_tmp['tokenized_into'].array\n",
    "    if len(tokenized_into):\n",
    "        tokenized_into = tokenized_into[0]\n",
    "    else:\n",
    "        tokenized_into = negative_affix\n",
    "    # print(tokenized_into)\n",
    "    row['tokenized_into'] = tokenized_into\n",
    "    \n",
    "    rows.append(row)\n",
    "df_merged = pd.DataFrame(data = rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59acd45-199e-43ce-a82f-91a42904aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0573963-c160-4b8c-8048-146b5541558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c862f0-b8f9-4aec-9e26-e0696a27a022",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['Counts'] = np.zeros(len(df_merged))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d643668-93e2-446b-8735-badae4ec8ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_by_model = df_merged.groupby(['tokenizer', 'negative_affix', 'tokenized_into', 'Predict'])[\"Predict\"].count().reset_index(name = \"count\")                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd2597-2431-4063-af91-002a0471cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_by_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d73836-7d43-4ab1-920d-89ef7ab7341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_by_model_total = df_merged_by_model.groupby(['tokenizer', 'negative_affix'])['count'].sum().reset_index(name = \"total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef39d30-ffc6-4e42-b60d-63c3904dcf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, row in df_merged_by_model.iterrows():\n",
    "    row['total'] = df_merged_by_model_total[df_merged_by_model_total['negative_affix'] == row['negative_affix']]['total'].iloc[0]\n",
    "    row['NegMorph'] = df_merged.loc[(df_merged['negative_affix'] == row['negative_affix']) & (df_merged['tokenized_into'] == row['tokenized_into'])]['NegMorph'].iloc[0] \n",
    "    rows.append(row)\n",
    "df_merged_by_model_total = pd.DataFrame(data = rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac12329c-f685-4a12-98b7-986c173b27a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_by_model_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4ef59e-e03f-4511-b762-1d6dbbaad08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_by_model_total_negmorph = df_merged_by_model_total.groupby(['tokenizer', 'negative_affix','NegMorph','Predict'])['count'].sum().reset_index(name = \"NegMorph_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b2f08-0c25-46f6-86b6-7ed7aade4d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_by_model_total_negmorph_percentage = df_merged_by_model_total_negmorph.groupby(['tokenizer','negative_affix','NegMorph'])['NegMorph_count'].sum().reset_index(name = 'total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c32317-2d11-43ca-a95a-b55cdb56850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_by_model_total_negmorph_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae5188-b3e1-48de-b900-78cfea5c2e18",
   "metadata": {},
   "source": [
    "#draw here\n",
    "percentage instead of count\n",
    "1 | 0\n",
    "for each 1: negmorph (percentage) negmorph_count/ total for that negative affix\n",
    "max 6 values (3 negmorph x 2 predict) for each\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad7cb5a-875c-42e9-ae89-ea07bf87adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_by_model_total_negmorph.to_csv('affix_performance_by_negmorph.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da386e-bb1a-49a8-810b-3a92e112e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_by_model_total_negmorph = pd.read_csv('affix_performance_by_negmorph.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2066545-3271-4c4b-ab82-5d0537046cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_by_model_total_negmorph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db25bde2-f6d3-4069-ae4a-6e24dfdb8199",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_by_model_total_affix = df_merged_by_model_total_negmorph.groupby(['tokenizer', 'negative_affix'])['NegMorph_count'].sum().reset_index(name = 'affix_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5724ad-be87-4bdf-b579-5ee532225ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_by_model_total_affix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b414bff-17a8-41a4-94a1-0ada4a4cbc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, row in df_merged_by_model_total_negmorph.iterrows():\n",
    "    affix = row['negative_affix']\n",
    "    tokenizer = row['tokenizer']\n",
    "    total = df_merged_by_model_total_affix.loc[(df_merged_by_model_total_affix['tokenizer'] == tokenizer) & (df_merged_by_model_total_affix['negative_affix'] == affix)]['affix_count'].iloc[0]\n",
    "    # print(total)\n",
    "    row['percentage'] = int(row['NegMorph_count']) / int(total)\n",
    "    # print(row['percentage'])\n",
    "    row['total'] = total\n",
    "\n",
    "    rows.append(row)\n",
    "    # print(row)\n",
    "    # print('================')\n",
    "# print(rows)\n",
    "df_merged_percentage = pd.DataFrame(data = rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aecbbe-4ddc-4028-a3c7-48e91e679d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121938ac-46d2-4f98-84ea-86421c413fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_percentage.to_csv('affix_negmorph_percentage.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0093cb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_percentage = pd.read_csv('affix_negmorph_percentage.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75011b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3291c8-e25a-426c-af12-853201d05c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_percentage.groupby(['tokenizer','negative_affix'])['percentage'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5c9e22-0327-49fd-9c06-6b4f8111ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_affixes_df = df_merged_percentage.sort_values('total', ascending=False).drop_duplicates('negative_affix').head(40)                                                                                                                  \n",
    "                                                                                                                                                                                                                    \n",
    "# Now we need to filter the original DataFrame to only include rows that correspond to the top 10 affixes                                                                                                             \n",
    "top_affixes_list = top_affixes_df['negative_affix'].tolist()                                                                                                                                                          \n",
    "filtered_df = df_merged_percentage[df_merged_percentage['negative_affix'].isin(top_affixes_list)]                                                                                                                                                         \n",
    "                                                                                                                                                                                                                    \n",
    "# Display the filtered DataFrame to verify the selection                                                                                                                                                              \n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74461bc-a309-4686-896a-5b766601b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_affixes_pivot = filtered_df.pivot_table(index='negative_affix', columns=['tokenizer','Predict','NegMorph'], values='percentage', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d7402a-2aaa-45e7-8745-a97047002f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_affixes_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b52810-3419-4a90-92c4-b137e21e5023",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_affixes_pivot.loc['mean'] = top_affixes_pivot.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af72fe85-cfbf-459d-9e65-23c093fdeff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_affixes_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6899e0-405f-4355-a05d-474d000aa5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_dict = {'model': 'GPT-4'}\n",
    "flan_dict = {'model': 'Flan-T5-xxl'}\n",
    "llama_dict = {'model': 'Llama-2-13B'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb749e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacaa0f4-2d70-45ef-8261-9c8b7d6d12dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = filtered_df['tokenizer'].unique()  \n",
    "# print(tokenizers)\n",
    "tokenizers = ['GPT-4','Flan-T5-xxl','LLama-2-13B']                                                                                                                                                                                                                        \n",
    "# Create a figure with subplots for each tokenizer                                                                                                                                                                    \n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(tokenizers), figsize=(10 * len(tokenizers), 15), sharex= True)                                                                                                              \n",
    "                                                                                                                                                                                                                    \n",
    "# Plot a stacked bar plot for each tokenizer                                                                                                                                                                          \n",
    "for ax, tokenizer in zip(axes, tokenizers):                                                                                                                                                                           \n",
    "  # Filter the DataFrame for the current tokenizer                                                                                                                                                                  \n",
    "    tokenizer_df = filtered_df[filtered_df['tokenizer'] == tokenizer]                                                                                                                         \n",
    "    # Pivot the data for the current tokenizer                                                                                                                                                                        \n",
    "    pivot_df = tokenizer_df.pivot_table(index='negative_affix',                                                                                                                                                       \n",
    "                                      columns=['Predict', 'NegMorph'],                                                                                                                                              \n",
    "                                      values='percentage',                                                                                                                                                          \n",
    "                                      fill_value=0)\n",
    "    pivot_df.loc['Overall'] = pivot_df.mean()\n",
    "    pivot_df = pivot_df[::-1]\n",
    "    if tokenizer == 'GPT-4':\n",
    "        gpt4_dict['NegMorph_Correct'] = pivot_df.loc['Overall'][0][0]\n",
    "        gpt4_dict['NegMorph_Over-segmented'] = pivot_df.loc['Overall'][0][1]\n",
    "        gpt4_dict['NegMorph_Under-segmented'] = pivot_df.loc['Overall'][0][2]\n",
    "    elif tokenizer == 'Flan-T5-xxl':\n",
    "        flan_dict['NegMorph_Correct'] = pivot_df.loc['Overall'][0][0]\n",
    "        flan_dict['NegMorph_Over-segmented'] = pivot_df.loc['Overall'][0][1]\n",
    "        flan_dict['NegMorph_Under-segmented'] = pivot_df.loc['Overall'][0][2]\n",
    "    elif tokenizer == 'LLama-2-13B':\n",
    "        llama_dict['NegMorph_Correct'] = pivot_df.loc['Overall'][0][0]\n",
    "        llama_dict['NegMorph_Over-segmented'] = pivot_df.loc['Overall'][0][1]\n",
    "        llama_dict['NegMorph_Under-segmented'] = pivot_df.loc['Overall'][0][2]\n",
    "\n",
    "        \n",
    "    print(pivot_df.loc['Overall'][0][0])\n",
    "    # print(pivot_df)\n",
    "  # Plot each part of the bar for Predict 0 and 1                                                                                                                                                                   \n",
    "    colors = {'Correct':'g', 'Under-segmented': 'y', 'Over-segmented': 'r'}\n",
    "    patterns = {0: '/', 1:'x'}\n",
    "    edges = {0: 'w' , 1: 'black'}\n",
    "    for predict in [0, 1]:\n",
    "        bottom = [0] * len(pivot_df)  # Initialize the bottom at 0 for each affix\n",
    "        for i,negmorph in enumerate(pivot_df.columns.levels[1]):\n",
    "            if predict == 1:\n",
    "                width = -pivot_df[(predict, negmorph)]\n",
    "                ax.barh(pivot_df.index, width = width, left=bottom,                                                                                                                                      \n",
    "                     label=negmorph, color = colors[negmorph], alpha = 0.6, edgecolor = 'white')                                                                                                                                                           \n",
    "                \n",
    "            else:\n",
    "                width = pivot_df[(predict, negmorph)]\n",
    "                ax.barh(pivot_df.index, width = width, left=bottom,                                                                                                                                      \n",
    "                     label=negmorph, color = colors[negmorph], alpha = 1, edgecolor = 'white')                                                                                                                                                           \n",
    "\n",
    "            # print(pivot_df.index)\n",
    "            # Update the bottom for the next NegMorph                                                                                                                                                                 \n",
    "            bottom += width.fillna(0)                                                                                                                                                         \n",
    "            # print(bottom)\n",
    "    bars = ax.patches\n",
    "    # print(len(bars))\n",
    "    for i in range(0, 246, 41):\n",
    "        bars[i].set_hatch('/')\n",
    "        bars[i].set_edgecolor('black')\n",
    "    # for i in range(42, 246, 41):\n",
    "    #     bars[i].set_color('x')\n",
    "    # Set the title for the current subplot                                                                                                                                                                           \n",
    "    ax.set_title(tokenizer)\n",
    "    ax.set_title(f'{tokenizer}', fontsize=30)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='y', which='major', labelsize=25)\n",
    "\n",
    "    # break\n",
    "                                                                                                                                                                                                                    \n",
    "# Set common labels                                                                                                                                                                                                   \n",
    "axes[1].set_xlabel('Percentage', fontsize=25) \n",
    "# ax.set_ylabel('Negative Affix')                                                                                                                                                                                       \n",
    "\n",
    "                                                                                                                                                                                                                    \n",
    "# Rotate x-axis labels for better readability                                                                                                                                                                         \n",
    "for ax in axes:                                                                                                                                                                                                       \n",
    "    ax.set_xticklabels(['1','','0.5','','0','','0.5','','1'], fontsize = 20)\n",
    "\n",
    "                                                                                                                                                                                                                    \n",
    "# Add legend                                                                                                                                                                                                          \n",
    "red_patch = mpatches.Patch(color='r', label='Over-segmented')\n",
    "yellow_patch = mpatches.Patch(color='y', label='Under-segmented')\n",
    "green_patch = mpatches.Patch(color='g', label='Correct')\n",
    "\n",
    "axes[1].legend(handles=[green_patch, yellow_patch,red_patch],fontsize=25, loc = 'upper center', bbox_to_anchor=(0.5, -0.05), ncol=3)\n",
    "# plt.axis('off')\n",
    "# plt.xticks([])\n",
    "                                                                                                                                                                                                        \n",
    "# Save the plot as an image                                                                                                                                                                                           \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('affix_negmorph_distribution_by_tokenizer.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e49384-87e8-4754-a5a9-cf98754201b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d9bc8c-2cf6-4999-87e7-7dcc52d60d3b",
   "metadata": {},
   "source": [
    "Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a8972c-a1eb-4745-8dfd-29863bcb92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd2fc65-e291-4531-a063-81d8dafce8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all: combine all metric to compare: neg non-neg all, 3 negmorph, 3 models(?) or overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561e7eca-0e07-4ee4-9641-55d2f338d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load accuracy into df_accuracy\n",
    "gpt4_pred = pd.read_csv('gpt4_pred.csv')\n",
    "flan_pred = pd.read_csv('flant5_pred.csv')\n",
    "llama2_pred = pd.read_csv('llama2_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7ff888-6025-4cbd-8812-d112c98207de",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce67be0-e339-4b85-9a96-8a7b2693c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9c1d5c-ca61-4734-b50a-e63d9714baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_accuracy = accuracy_score(gpt4_pred['gold'], gpt4_pred['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafdbf20-c7b4-4c03-a73b-a64dd96dd545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_acc(df):\n",
    "    df_neg = df[df['gold'] == 0]\n",
    "    acc_neg = accuracy_score(df_neg['gold'], df_neg['pred'])\n",
    "    df_pos = df[df['gold'] == 1]\n",
    "    acc_pos = accuracy_score(df_pos['gold'], df_pos['pred'])\n",
    "    acc = accuracy_score(df['gold'], df['pred'])\n",
    "    return {'acc_neg':acc_neg, 'acc_pos':acc_pos, 'acc_all':acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547c0ec6-0eaf-478f-b892-1380b91f1529",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b47e091-f14d-48c2-ab8a-c0df5b71978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_dict.update(calculate_acc(gpt4_pred))\n",
    "\n",
    "flan_dict.update(calculate_acc(flan_pred))\n",
    "# flan_dict['model'] = 'FLan-T5-xxl'\n",
    "llama_dict.update(calculate_acc(llama2_pred))\n",
    "# llama_dict['model'] = 'Llama-2-13B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ba38b3-95aa-44ac-a22b-591d2342646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc24c17e-a3ad-4c86-8116-49807c467058",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [gpt4_dict, flan_dict, llama_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023dc10a-77c7-4eaf-9646-7d5c8353ec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame(data = results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27097e9-0ab1-4bae-bea9-efbbce8e5ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc70dbdd-24af-4559-b8fa-6ea022df3b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dbf7b8-e97e-4675-af0a-8f63c585f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_metrics = ['acc_neg']\n",
    "negmorph_metrics = ['NegMorph_Correct', 'NegMorph_Over-segmented', 'NegMorph_Under-segmented']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69367f3-56fe-4ccf-9d5e-27c6edfb0bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b908cd2-ed57-4fca-8c39-fcf9f361aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badbd519-8f92-462f-b14f-511e1bd4932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_func(x,y):\n",
    "    return pearsonr(x,y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fc873f-ef1d-404a-9ca5-358ee3287e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f7b3e3-6398-4695-8769-6a8dbff8f54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrfunc(x, y, hue=None, ax=None, **kws):\n",
    "    \"\"\"Plot the correlation coefficient in the top left hand corner of a plot.\"\"\"\n",
    "    try:\n",
    "        # print(method)\n",
    "        # method = stats.PermutationMethod(n_resamples=np.inf, random_state=rng)\n",
    "        # method = stats.MonteCarloMethod(rvs=(rng.uniform, rng.uniform))\n",
    "\n",
    "        r, p = pearsonr(x, y)\n",
    "        print(r,p)\n",
    "    except:\n",
    "        r = -1\n",
    "        p = 0.0\n",
    "    # print(r,p)\n",
    "    # res = bootstrap((x,y), pearson_func , vectorized = False, paired = True)\n",
    "    # print(res.confidence_interval)\n",
    "    # print(res)\n",
    "    # method = stats.BootstrapMethod(method='BCa', random_state=rng)\n",
    "    # print(pearsonr(x, y).confidence_interval(confidence_level=0.9, method=method))\n",
    "    if p <= 0.05:\n",
    "        add_asterisk = '*'\n",
    "    else:\n",
    "        add_asterisk = ''\n",
    "    ax = ax or plt.gca()\n",
    "    ax.annotate(f'r = {r:.3f}{add_asterisk}', xy=(0.1, 0.97), xycoords=ax.transAxes)\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "sns.set_style(\"whitegrid\", {'axes.grid': False})\n",
    "\n",
    "g = sns.pairplot(\n",
    "    df_all, \n",
    "    # diag_kind=\"kde\",\n",
    "    y_vars = acc_metrics,\n",
    "    x_vars = negmorph_metrics,\n",
    "    kind=\"reg\", \n",
    "    plot_kws={\n",
    "        'line_kws':{'color':'red'},\n",
    "        'scatter_kws': {'s': 2}\n",
    "    },\n",
    "    # corner = True,\n",
    "    height = 2\n",
    "    # hue = 'category'\n",
    "    \n",
    ")\n",
    "g.map(corrfunc)\n",
    "\n",
    "acc_metrics_label = ['Neg']\n",
    "negmorph_metrics_label = ['Correct', 'Over-segmented', 'Under-segmented']\n",
    "\n",
    "for i in range(len(negmorph_metrics)):\n",
    "    for j in range(len(acc_metrics)):\n",
    "        g.axes[j,i].xaxis.set_label_text(negmorph_metrics_label[i])\n",
    "        g.axes[j,i].yaxis.set_label_text(acc_metrics_label[j])\n",
    "\n",
    "# g.set( ylabel = 'Accuracy')\n",
    "\n",
    "g.tight_layout()\n",
    "# g.axes[0,0].set_xlim((0.0,0.8))\n",
    "# g.axes[0,1].set_xlim((-0.1,1.0))\n",
    "# g.axes[0,2].set_xlim((-0.1,1.0))\n",
    "# g.axes[0,3].set_xlim((-0.1,1.0))\n",
    "# g.axes[0,4].set_xlim((-0.1,1.0))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a397fe-7238-4b85-9e4a-86d623ea4ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8971c449",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
